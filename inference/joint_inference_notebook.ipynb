{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEDDEx Inference\n",
    "\n",
    "This notebook is meant to provide an interactive API for inference on the joint model to predict terms, abbreviations, symbols and their definitions etc. There are two meta tasks that can be accomplished by these models.\n",
    "### Meta Task 1 (Slot Prediction)\n",
    "- Intent prediction per sentence and Slot prediction per token of the sentence for the tasks the model was trained on (AI2020, DocDef, W00, etc.)\n",
    "- Example input : `The input to the matrix [[M]] is a vector [[v]] of specified lengths.`\n",
    "- Example output : {''}\n",
    "\n",
    "### Meta Task 2 (Query based Prediction)\n",
    "- These models are trained to predict the nicknames for symbols (DocDef dataset), given a special query token around the symbol whose nickname we want predicted\n",
    "- Example input : `The input to the matrix [[M]] is a vector </s> [[v]] </s> of specified lengths.`\n",
    "- Example output : {''}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nuwandavek/anaconda3/envs/scholarphi/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append('../')\n",
    "from model import JointRoberta\n",
    "from transformers import HfArgumentParser, AutoTokenizer\n",
    "from configuration import (DataTrainingArguments,ModelArguments,TrainingArguments,)\n",
    "from utils import get_joint_labels\n",
    "from trainer_joint import JointTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the variables here are correct\n",
    "# models_dir_path = './models/DocDef2+AI2020+W00_model'\n",
    "# models_dir_path = \"./models/joint_abbrexp_termdef_symnick\"\n",
    "models_dir_path = \"./models/AI2020_model\"\n",
    "# task = \"DocDef2+AI2020+W00\"\n",
    "task = \"AI2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\",\n",
    "    \"roberta-large\",\n",
    "    \"--data_dir\",\n",
    "    models_dir_path,\n",
    "    \"--task\",\n",
    "    task,\n",
    "    \"--output_dir\",\n",
    "    f\"{models_dir_path}/roberta-large\",\n",
    "    \"--use_crf\",\n",
    "    \"--use_heuristic\",\n",
    "    \"--use_pos\",\n",
    "    \"--use_np\",\n",
    "    \"--use_vp\",\n",
    "    \"--use_entity\",\n",
    "    \"--use_acronym\",\n",
    "    \"--do_train\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    \"4\",\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    \"4\",\n",
    "    \"--max_seq_len\",\n",
    "    \"80\"])\n",
    "training_args.output_dir = \"{}{}{}{}{}{}\".format(\n",
    "            training_args.output_dir,\n",
    "            \"_pos={}\".format(training_args.use_pos)\n",
    "            if training_args.use_pos\n",
    "            else \"\",\n",
    "            \"_np={}\".format(training_args.use_np) if training_args.use_np else \"\",\n",
    "            \"_vp={}\".format(training_args.use_vp) if training_args.use_vp else \"\",\n",
    "            \"_entity={}\".format(training_args.use_entity)\n",
    "            if training_args.use_entity\n",
    "            else \"\",\n",
    "            \"_acronym={}\".format(training_args.use_acronym)\n",
    "            if training_args.use_acronym\n",
    "            else \"\",\n",
    ")\n",
    "os.environ[\"WANDB_DISABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /home/nuwandavek/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/nuwandavek/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/nuwandavek/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "loading configuration file ./models/AI2020_model/roberta-large_pos=True_np=True_vp=True_entity=True_acronym=True/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"JointRoberta\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models/AI2020_model/roberta-large_pos=True_np=True_vp=True_entity=True_acronym=True/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing JointRoberta.\n",
      "\n",
      "All the weights of JointRoberta were initialized from the model checkpoint at ./models/AI2020_model/roberta-large_pos=True_np=True_vp=True_entity=True_acronym=True.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use JointRoberta for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "intent_label_dict = get_joint_labels(training_args, \"intent_label\")\n",
    "slot_label_dict = get_joint_labels(training_args, \"slot_label\")\n",
    "pos_label_lst = get_joint_labels(training_args, \"pos_label\")\n",
    "\n",
    "model = JointRoberta.from_pretrained(\n",
    "    training_args.output_dir,\n",
    "    args=training_args,\n",
    "    intent_label_dict=intent_label_dict,\n",
    "    slot_label_dict=slot_label_dict,\n",
    "    pos_label_lst=pos_label_lst,\n",
    "    tasks=task.split('+'),\n",
    ")\n",
    "\n",
    "trainer = JointTrainer([training_args, model_args, data_args,],model,tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting example 0 of 1\n",
      "*** Example ***\n",
      "guid: one\n",
      "tokens: <s> In order to d ynam ically learn fil ters and features we look to Con v olution al Ne ural Net works ( CNN s ) which have shown their dom inance in computer vision C itation ( C IT ATION ) . </s>\n",
      "input_ids: 0 1121 10337 560 417 39116 3435 38229 18419 2696 463 46076 1694 13724 560 9157 705 23794 337 14563 9799 15721 11655 1640 16256 29 43 5488 11990 44270 25017 12623 13598 179 36327 14675 347 12257 1640 347 2068 6034 43 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "intent_label: 1 (id = 1)\n",
      "slot_labels: 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'AI2020': [1]}, {'AI2020': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DEF', 'DEF', 'DEF', 'O', 'TERM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DEF', 'DEF', 'DEF', 'O', 'TERM', 'O', 'O']]}, {'AI2020': [[0.99997866, 0.999969, 0.99998283, 0.99994326, 0.9999665, 0.9999585, 0.99998, 0.99997663, 0.99997556, 0.9999653, 0.99997437, 0.9998549, 0.99977964, 0.9998603, 0.99975663, 0.99949634, 0.9999597, 0.9999745, 0.9999821, 0.99997866, 0.9999826, 0.9999548, 0.9999634, 0.9771666, 0.9833735, 0.9893351, 0.9998086, 0.9968988, 0.99988735, 0.95342153]]})\n"
     ]
    }
   ],
   "source": [
    "text = \"In order to dynamically learn filters and features we look to Convolutional Neural Networks (CNNs) which have shown their dominance in computer vision Citation ( CITATION ) .\"\n",
    "print(trainer.predict_batch([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting example 0 of 1\n",
      "*** Example ***\n",
      "guid: one\n",
      "tokens: <s> In order to d ynam ically learn fil ters and features we look to Con v olution al Ne ural Net works ( CNN s ) which have shown their dom inance in computer vision </s>\n",
      "input_ids: 0 1121 10337 560 417 39116 3435 38229 18419 2696 463 46076 1694 13724 560 9157 705 23794 337 14563 9799 15721 11655 1640 16256 29 43 5488 11990 44270 25017 12623 13598 179 36327 14675 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "intent_label: 1 (id = 1)\n",
      "slot_labels: 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'AI2020': [1]}, {'AI2020': [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DEF', 'DEF', 'DEF', 'O', 'TERM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]}, {'AI2020': [[0.9999759, 0.9999647, 0.9999778, 0.9999305, 0.999961, 0.99995065, 0.9999795, 0.9999751, 0.9999726, 0.9999577, 0.9999759, 0.9998479, 0.9998282, 0.99986744, 0.9997521, 0.99921274, 0.9999627, 0.99997413, 0.99998164, 0.9999789, 0.9999814, 0.99994683, 0.9999844, 0.9999691, 0.9999796]]})\n"
     ]
    }
   ],
   "source": [
    "text = \"In order to dynamically learn filters and features we look to Convolutional Neural Networks (CNNs) which have shown their dominance in computer vision\"\n",
    "print(trainer.predict_batch([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
